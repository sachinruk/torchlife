{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "> Functions used to create pytorch `DataSet`s and `DataLoader`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from fastai.data_block import DataBunch, DatasetType\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_breakpoints(df, percentiles:list=[20, 40, 60, 80]):\n",
    "    \"\"\"\n",
    "    Gives the times at which death events occur at given percentile\n",
    "    parameters:\n",
    "    df - must contain columns 't' (time) and 'e' (death event)\n",
    "    percentiles - list of percentages at which breakpoints occur (do not include 0 and 100)\n",
    "    \"\"\"\n",
    "    event_times = df.loc[df['e']==1, 't'].values\n",
    "    breakpoints = np.percentile(event_times, percentiles)\n",
    "    breakpoints = [0] + breakpoints.tolist() + [df['t'].max()]\n",
    "    \n",
    "    widths = np.diff(breakpoints).tolist()\n",
    "    return breakpoints, widths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14.600000000000001, 24.0, 35.0, 43.400000000000006]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/CamDavidsonPilon/lifelines/master/lifelines/datasets/rossi.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.rename(columns={'week':'t', 'arrest':'e'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TestData(Dataset):\n",
    "    \"\"\"\n",
    "    Create pyTorch Dataset\n",
    "    parameters:\n",
    "    - t: time elapsed\n",
    "    - b: (optional) breakpoints where the hazard is different to previous segment of time. \n",
    "    **Must include 0 as first element and the maximum time as last element**\n",
    "    - x: (optional) features\n",
    "    \"\"\"\n",
    "    def __init__(self, t, b:list=None, x=None):\n",
    "        super().__init__()\n",
    "        assert isinstance(b, np.ndarray) or isinstance(b, list) or b is None\\\n",
    "                , \"Breakpoints need to be a list\"\n",
    "        self.t, self.b, self.x = t, b, x\n",
    "        if b:\n",
    "            self.b = b[1:-1]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.t)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        time = torch.Tensor([self.t[i]])\n",
    "        \n",
    "        if self.b is None:\n",
    "            x_ = (time,)\n",
    "        else:\n",
    "            t_section = torch.LongTensor([np.searchsorted(self.b, self.t[i])])\n",
    "            x_ = (time, t_section.squeeze())\n",
    "        \n",
    "        if self.x is not None:\n",
    "            x = torch.Tensor(self.x[i])\n",
    "            x_ = x_ + (x,)\n",
    "            \n",
    "        return x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Data(TestData):\n",
    "    \"\"\"\n",
    "    Create pyTorch Dataset\n",
    "    parameters:\n",
    "    - t: time elapsed\n",
    "    - e: (death) event observed. 1 if observed, 0 otherwise.\n",
    "    - b: (optional) breakpoints where the hazard is different to previous segment of time.\n",
    "    - x: (optional) features\n",
    "    \"\"\"\n",
    "    def __init__(self, t, e, b=None, x=None):\n",
    "        super().__init__(t, b, x)\n",
    "        self.e = e\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        x_ = super().__getitem__(i)\n",
    "        e = torch.Tensor([self.e[i]])\n",
    "        return x_, e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([torch.Size([64, 1]), torch.Size([64, 3])], torch.Size([64, 1]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "np.random.seed(42)\n",
    "N = 100\n",
    "D = 3\n",
    "p = 0.1\n",
    "bs = 64\n",
    "\n",
    "x = np.random.randn(N, D)\n",
    "t = np.arange(N)\n",
    "e = np.random.binomial(1, p, N)\n",
    "\n",
    "data = Data(t, e, x=x)\n",
    "batch = next(iter(DataLoader(data, bs)))\n",
    "assert len(batch[-1]) == bs, (f\"length of batch {len(batch)} is different\" \n",
    "                          f\"to intended batch size {bs}\")\n",
    "[b.shape for b in batch[0]], batch[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([64, 1]), torch.Size([64]), torch.Size([64, 3])] torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "N = 100\n",
    "D = 3\n",
    "p = 0.1\n",
    "bs = 64\n",
    "breakpoints = [10, 50]\n",
    "\n",
    "data = Data(t, e, breakpoints, x)\n",
    "batch2 = next(iter(DataLoader(data, bs)))\n",
    "assert len(batch2[-1]) == bs, (f\"length of batch {len(batch2)} is different\" \n",
    "                          f\"to intended batch size {bs}\")\n",
    "print([b.shape for b in batch2[0]], batch2[1].shape)\n",
    "\n",
    "assert torch.all(batch[0][0] == batch2[0][0]), (\"Discrepancy between batch \"\n",
    "                                                \"with breakpoints and without\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TestDataFrame(TestData):\n",
    "    \"\"\"\n",
    "    Wrapper around Data Class that takes in a dataframe instead\n",
    "    parameters:\n",
    "    - df: dataframe. **Must have t (time) and e (event) columns, other cols optional.\n",
    "    - b: breakpoints of time (optional)\n",
    "    \"\"\"\n",
    "    def __init__(self, df, b=None):\n",
    "        t = df['t'].values\n",
    "        remainder = list(set(df.columns) - set(['t', 'e']))\n",
    "        x = df[remainder].values\n",
    "        if x.shape[1] == 0:\n",
    "            x = None\n",
    "        super().__init__(t, b, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DataFrame(Data):\n",
    "    \"\"\"\n",
    "    Wrapper around Data Class that takes in a dataframe instead\n",
    "    parameters:\n",
    "    - df: dataframe. **Must have t (time) and e (event) columns, other cols optional.\n",
    "    - b: breakpoints of time (optional)\n",
    "    \"\"\"\n",
    "    def __init__(self, df, b=None):\n",
    "        t = df['t'].values\n",
    "        e = df['e'].values\n",
    "        x = df.drop(['t', 'e'], axis=1).values\n",
    "        if x.shape[1] == 0:\n",
    "            x = None\n",
    "        super().__init__(t, e, b, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([1.]),), tensor([0.]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "# testing with pandas dataframe\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'t': t, 'e': e})\n",
    "df2 = DataFrame(df)\n",
    "df2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([1.]), tensor([ 1.5230, -0.2342, -0.2341])), tensor([0.]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "# testing with x\n",
    "new_df = pd.concat([df, pd.DataFrame(x)], axis=1)\n",
    "df3 = DataFrame(new_df)\n",
    "df3[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([1.]), tensor(0), tensor([ 1.5230, -0.2342, -0.2341])), tensor([0.]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "# testing with breakpoints\n",
    "new_df = pd.concat([df, pd.DataFrame(x)], axis=1)\n",
    "df3 = DataFrame(new_df, breakpoints)\n",
    "df3[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create iterable data loaders/ fastai databunch using above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def create_db(df, b=None, train_p=0.8, bs=128):\n",
    "    \"\"\"\n",
    "    Take dataframe and split into train, test, val (optional)\n",
    "    and convert to Fastai databunch\n",
    "\n",
    "    parameters:\n",
    "    - df: pandas dataframe\n",
    "    - b: breakpoints of time (optional)\n",
    "    - train_p: training percentage\n",
    "    - bs: batch size\n",
    "    \"\"\"\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    train_len = int(train_p*len(df))\n",
    "    train_ds = DataFrame(df.iloc[:train_len], b)\n",
    "    val_ds = DataFrame(df.iloc[train_len:], b)\n",
    "    \n",
    "    train_dl = DataLoader(train_ds, bs, shuffle=True, drop_last=False)\n",
    "    val_dl = DataLoader(val_ds, bs, drop_last=False)\n",
    "    db = DataBunch(train_dl, val_dl)\n",
    "    \n",
    "    return db\n",
    "\n",
    "def create_test_dl(df, b=None, bs=128):\n",
    "    \"\"\"\n",
    "    Take dataframe and return a pytorch dataloader.\n",
    "    parameters:\n",
    "    - df: pandas dataframe\n",
    "    - b: breakpoints of time (optional)\n",
    "    - bs: batch size\n",
    "    \"\"\"\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    test_ds = TestDataFrame(df, b)\n",
    "    test_dl = DataLoader(test_ds, bs, shuffle=False, drop_last=False)\n",
    "    return test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_index.ipynb.\n",
      "Converted 10_SAT.ipynb.\n",
      "Converted 20_KaplanMeier.ipynb.\n",
      "Converted 50_hazard.ipynb.\n",
      "Converted 55_hazard.PiecewiseHazard.ipynb.\n",
      "Converted 59_hazard.Cox.ipynb.\n",
      "Converted 60_AFT_models.ipynb.\n",
      "Converted 65_AFT_error_distributions.ipynb.\n",
      "Converted 80_data.ipynb.\n",
      "Converted 90_model.ipynb.\n",
      "Converted 95_Losses.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
