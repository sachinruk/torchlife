{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "> Functions used to create pytorch `DataSet`s and `DataLoader`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from fastai.data_block import DataBunch, DatasetType\n",
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import MaxAbsScaler, StandardScaler\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/CamDavidsonPilon/lifelines/master/lifelines/datasets/rossi.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.rename(columns={'week':'t', 'arrest':'e'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "isinstance(MaxAbsScaler(), sklearn.preprocessing._data.MaxAbsScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TestData(Dataset):\n",
    "    \"\"\"\n",
    "    Create pyTorch Dataset\n",
    "    parameters:\n",
    "    - t: time elapsed\n",
    "    - b: (optional) breakpoints where the hazard is different to previous segment of time. \n",
    "    **Must include 0 as first element and the maximum time as last element**\n",
    "    - x: (optional) features\n",
    "    \"\"\"\n",
    "    def __init__(self, t:np.array, b:Optional[np.array]=None, x:Optional[np.array]=None, \n",
    "                 t_scaler:MaxAbsScaler=None, x_scaler:StandardScaler=None) -> None:\n",
    "        super().__init__()\n",
    "        self.t, self.b, self.x = t, b, x\n",
    "        if len(t.shape) == 1:\n",
    "            self.t = t[:,None]\n",
    "\n",
    "#         breakpoint()\n",
    "        if t_scaler:\n",
    "            self.t_scale = t_scaler\n",
    "            self.t = self.t_scale.transform(self.t)\n",
    "        else:\n",
    "            self.t_scale = MaxAbsScaler()\n",
    "            self.t = self.t_scale.fit_transform(self.t)\n",
    "        \n",
    "        if b is not None:\n",
    "            b = b[1:-1]\n",
    "            if len(b.shape) == 1:\n",
    "                b = b[:,None]\n",
    "            if t_scaler:\n",
    "                self.b = t_scaler.transform(b).squeeze()\n",
    "            else:\n",
    "                self.b = self.t_scale.transform(b).squeeze()\n",
    "            \n",
    "        if x is not None:\n",
    "            if len(x.shape) == 1:\n",
    "                self.x = x[:,None]\n",
    "            if x_scaler:\n",
    "                self.x_scale = x_scaler\n",
    "                self.x = self.x_scale.transform(self.x)\n",
    "            else:\n",
    "                self.x_scale = StandardScaler()\n",
    "                self.x = self.x_scale.fit_transform(self.x)\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.t)\n",
    "    \n",
    "    def __getitem__(self, i:int) -> Tuple:\n",
    "        time = torch.Tensor(self.t[i])\n",
    "        \n",
    "        if self.b is None:\n",
    "            x_ = (time,)\n",
    "        else:\n",
    "            t_section = torch.LongTensor([np.searchsorted(self.b, self.t[i])])\n",
    "            x_ = (time, t_section.squeeze())\n",
    "        \n",
    "        if self.x is not None:\n",
    "            x = torch.Tensor(self.x[i])\n",
    "            x_ = x_ + (x,)\n",
    "            \n",
    "        return x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Data(TestData):\n",
    "    \"\"\"\n",
    "    Create pyTorch Dataset\n",
    "    parameters:\n",
    "    - t: time elapsed\n",
    "    - e: (death) event observed. 1 if observed, 0 otherwise.\n",
    "    - b: (optional) breakpoints where the hazard is different to previous segment of time.\n",
    "    - x: (optional) features\n",
    "    \"\"\"\n",
    "    def __init__(self, t:np.array, e:np.array, b:Optional[np.array]=None, x:Optional[np.array]=None,\n",
    "                t_scaler:MaxAbsScaler=None, x_scaler:StandardScaler=None) -> None:\n",
    "        super().__init__(t, b, x, t_scaler, x_scaler)\n",
    "        self.e = e\n",
    "        if len(e.shape) == 1:\n",
    "            self.e = e[:,None]\n",
    "        \n",
    "    def __getitem__(self, i) -> Tuple:\n",
    "        x_ = super().__getitem__(i)\n",
    "        e = torch.Tensor(self.e[i])\n",
    "        return x_, e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([torch.Size([64, 1]), torch.Size([64, 3])], torch.Size([64, 1]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "np.random.seed(42)\n",
    "N = 100\n",
    "D = 3\n",
    "p = 0.1\n",
    "bs = 64\n",
    "\n",
    "x = np.random.randn(N, D)\n",
    "t = np.arange(N)\n",
    "e = np.random.binomial(1, p, N)\n",
    "\n",
    "data = Data(t, e, x=x)\n",
    "batch = next(iter(DataLoader(data, bs)))\n",
    "assert len(batch[-1]) == bs, (f\"length of batch {len(batch)} is different\" \n",
    "                          f\"to intended batch size {bs}\")\n",
    "[b.shape for b in batch[0]], batch[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([64, 1]), torch.Size([64]), torch.Size([64, 3])] torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "breakpoints = np.array([0, 10, 50, N-1])\n",
    "\n",
    "data = Data(t, e, breakpoints, x)\n",
    "batch2 = next(iter(DataLoader(data, bs)))\n",
    "assert len(batch2[-1]) == bs, (f\"length of batch {len(batch2)} is different\" \n",
    "                          f\"to intended batch size {bs}\")\n",
    "print([b.shape for b in batch2[0]], batch2[1].shape)\n",
    "\n",
    "assert torch.all(batch[0][0] == batch2[0][0]), (\"Discrepancy between batch \"\n",
    "                                                \"with breakpoints and without\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TestDataFrame(TestData):\n",
    "    \"\"\"\n",
    "    Wrapper around Data Class that takes in a dataframe instead\n",
    "    parameters:\n",
    "    - df: dataframe. **Must have t (time) and e (event) columns, other cols optional.\n",
    "    - b: breakpoints of time (optional)\n",
    "    \"\"\"\n",
    "    def __init__(self, df:DataFrame, b:Optional[np.array]=None,\n",
    "                 t_scaler:MaxAbsScaler=None, x_scaler:StandardScaler=None) -> None:\n",
    "        t = df['t'].values\n",
    "        remainder = list(set(df.columns) - set(['t', 'e']))\n",
    "        x = df[remainder].values\n",
    "        if x.shape[1] == 0:\n",
    "            x = None\n",
    "        super().__init__(t, b, x, t_scaler, x_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DataFrame(Data):\n",
    "    \"\"\"\n",
    "    Wrapper around Data Class that takes in a dataframe instead\n",
    "    parameters:\n",
    "    - df: dataframe. **Must have t (time) and e (event) columns, other cols optional.\n",
    "    - b: breakpoints of time (optional)\n",
    "    \"\"\"\n",
    "    def __init__(self, df:DataFrame, b:Optional[np.array]=None,\n",
    "                t_scaler:MaxAbsScaler=None, x_scaler:StandardScaler=None) -> None:\n",
    "        t = df['t'].values\n",
    "        e = df['e'].values\n",
    "        x = df.drop(['t', 'e'], axis=1).values\n",
    "        if x.shape[1] == 0:\n",
    "            x = None\n",
    "        super().__init__(t, e, b, x, t_scaler, x_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([0.0101]),), tensor([0.]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "# testing with pandas dataframe\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'t': t, 'e': e})\n",
    "df2 = DataFrame(df)\n",
    "df2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([0.0101]), tensor([ 1.7440, -0.0523, -0.2790])), tensor([0.]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "# testing with x\n",
    "new_df = pd.concat([df, pd.DataFrame(x)], axis=1)\n",
    "df3 = DataFrame(new_df)\n",
    "df3[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([0.0101]), tensor(0), tensor([ 1.7440, -0.0523, -0.2790])),\n",
       " tensor([0.]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "# testing with breakpoints\n",
    "new_df = pd.concat([df, pd.DataFrame(x)], axis=1)\n",
    "df3 = DataFrame(new_df, breakpoints)\n",
    "df3[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create iterable data loaders/ fastai databunch using above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def create_db(df, b:Optional[np.array]=None, train_p:float=0.8, bs:int=128) -> None:\n",
    "    \"\"\"\n",
    "    Take dataframe and split into train, test, val (optional)\n",
    "    and convert to Fastai databunch\n",
    "\n",
    "    parameters:\n",
    "    - df: pandas dataframe\n",
    "    - b: breakpoints of time (optional)\n",
    "    - train_p: training percentage\n",
    "    - bs: batch size\n",
    "    \"\"\"\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    train_len = int(train_p*len(df))\n",
    "    \n",
    "    train_ds = DataFrame(df.iloc[:train_len], b)\n",
    "    val_ds = DataFrame(df.iloc[train_len:], b, train_ds.t_scale, train_ds.x_scale)\n",
    "    \n",
    "    train_dl = DataLoader(train_ds, bs, shuffle=True, drop_last=False)\n",
    "    val_dl = DataLoader(val_ds, bs, drop_last=False)\n",
    "    db = DataBunch(train_dl, val_dl)\n",
    "    \n",
    "    if b is None:\n",
    "        return db\n",
    "    else:\n",
    "        return db, train_ds.t_scale\n",
    "\n",
    "def create_test_dl(df, t_scaler:MaxAbsScaler, b:Optional[np.array]=None, bs:int=128,\n",
    "                  x_scaler:StandardScaler=None) -> None:\n",
    "    \"\"\"\n",
    "    Take dataframe and return a pytorch dataloader.\n",
    "    parameters:\n",
    "    - df: pandas dataframe\n",
    "    - b: breakpoints of time (optional)\n",
    "    - bs: batch size\n",
    "    \"\"\"\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    test_ds = TestDataFrame(df, b, t_scaler, x_scaler)\n",
    "    test_dl = DataLoader(test_ds, bs, shuffle=False, drop_last=False)\n",
    "    return test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_breakpoints(df:DataFrame, percentiles:list=[20, 40, 60, 80]) -> np.array:\n",
    "    \"\"\"\n",
    "    Gives the times at which death events occur at given percentile\n",
    "    parameters:\n",
    "    df - must contain columns 't' (time) and 'e' (death event)\n",
    "    percentiles - list of percentages at which breakpoints occur (do not include 0 and 100)\n",
    "    \"\"\"\n",
    "    event_times = df.loc[df['e']==1, 't'].values\n",
    "    breakpoints = np.percentile(event_times, percentiles)\n",
    "    breakpoints = np.array([0] + breakpoints.tolist() + [df['t'].max()])\n",
    "    \n",
    "    return breakpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_index.ipynb.\n",
      "Converted 10_SAT.ipynb.\n",
      "Converted 20_KaplanMeier.ipynb.\n",
      "Converted 50_hazard.ipynb.\n",
      "Converted 55_hazard.PiecewiseHazard.ipynb.\n",
      "Converted 59_hazard.Cox.ipynb.\n",
      "Converted 60_AFT_models.ipynb.\n",
      "Converted 65_AFT_error_distributions.ipynb.\n",
      "Converted 80_data.ipynb.\n",
      "Converted 90_model.ipynb.\n",
      "Converted 95_Losses.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
