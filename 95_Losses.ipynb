{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losses\n",
    "> All the losses used in SA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from typing import Tuple\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we have:\n",
    "$$\n",
    "t_i = \\mu + \\xi_i\n",
    "$$\n",
    "and $\\xi_i\\sim p(\\xi_i|\\theta)$. Then $\\xi_i|\\mu\\sim p_\\mu(\\xi_i|\\theta)$ where $p_\\mu(\\xi_i|\\theta)$ is simply the distribution $p(\\xi_i|\\theta)$ shifted to the left by $\\mu$.\n",
    "\n",
    "In the event that the event is censored ($e_i=0$), we know that $t_i < \\mu + \\xi_i$ since the 'death' offset of $\\xi_i$ is not observed. \n",
    "\n",
    "Therefore we may write the likelihood of \n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(t_i, e_i|\\mu) =& \\left(p(t_i-\\mu)\\right)^{e_i} \\left(\\int_{t_i}^\\infty p(t-\\mu) dt\\right)^{1-e_i}\\\\\n",
    "\\log p(t_i, e_i|\\mu) =& e_i \\log p(t-\\mu) + (1 - e_i) \\log \\left(1 - \\int_{-\\infty}^{t_i} p(t-\\mu) dt \\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _aft_loss(log_pdf, log_cdf, e):\n",
    "    lik = e * log_pdf + (1 - e) * log_cdf\n",
    "    return -lik.mean()\n",
    "\n",
    "def aft_loss(params, e):\n",
    "    log_pdf, log_cdf = params\n",
    "    return _aft_loss(log_pdf, log_cdf, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the following loss function to infer our model. See [here](./SAT#Likelihood-Function) for theory.\n",
    "$$\n",
    "-\\log L = \\sum_{i=1}^N \\Lambda(t_i) - d_i \\log \\lambda(t_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _hazard_loss(logλ, Λ, e):\n",
    "    log_lik = e * logλ - Λ\n",
    "    return -log_lik.mean()\n",
    "\n",
    "def hazard_loss(hazard:Tuple[Tensor, Tensor], e:Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    parameters:\n",
    "    - hazard: log hazard and Cumulative hazard\n",
    "    - e: tensor of 1 if death event occured and 0 otherwise\n",
    "    \"\"\"\n",
    "    logλ, Λ = hazard\n",
    "    return _hazard_loss(logλ, Λ, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_index.ipynb.\n",
      "Converted 10_SAT.ipynb.\n",
      "Converted 20_KaplanMeier.ipynb.\n",
      "Converted 50_hazard.ipynb.\n",
      "Converted 55_hazard.PiecewiseHazard.ipynb.\n",
      "Converted 59_hazard.Cox.ipynb.\n",
      "Converted 60_AFT_models.ipynb.\n",
      "Converted 65_AFT_error_distributions.ipynb.\n",
      "Converted 80_data.ipynb.\n",
      "Converted 90_model.ipynb.\n",
      "Converted 95_Losses.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
