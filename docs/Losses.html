---

title: Losses

keywords: fastai
sidebar: home_sidebar

summary: "All the losses used in SA."
description: "All the losses used in SA."
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: 95_Losses.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Suppose that we have:
$$
t_i = \mu + \xi_i
$$
and $\xi_i\sim p(\xi_i|\theta)$. Then $\xi_i|\mu\sim p_\mu(\xi_i|\theta)$ where $p_\mu(\xi_i|\theta)$ is simply the distribution $p(\xi_i|\theta)$ shifted to the left by $\mu$.</p>
<p>In the event that the event is censored ($e_i=0$), we know that $t_i &lt; \mu + \xi_i$ since the 'death' offset of $\xi_i$ is not observed.</p>
<p>Therefore we may write the likelihood of 
$$
\begin{aligned}
p(t_i, e_i|\mu) =&amp; \left(p(t_i-\mu)\right)^{e_i} \left(\int_{t_i}^\infty p(t-\mu) dt\right)^{1-e_i}\\
\log p(t_i, e_i|\mu) =&amp; e_i \log p(t-\mu) + (1 - e_i) \log \left(1 - \int_{-\infty}^{t_i} p(t-\mu) dt \right)
\end{aligned}
$$</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="aft_loss" class="doc_header"><code>aft_loss</code><a href="https://github.com/sachinruk/torchlife/tree/master/torchlife/losses.py#L14" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>aft_loss</code>(<strong><code>params</code></strong>, <strong><code>e</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We use the following loss function to infer our model. See <a href="./SAT#Likelihood-Function">here</a> for theory.
$$
-\log L = \sum_{i=1}^N \Lambda(t_i) - d_i \log \lambda(t_i)
$$</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="hazard_loss" class="doc_header"><code>hazard_loss</code><a href="https://github.com/sachinruk/torchlife/tree/master/torchlife/losses.py#L23" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>hazard_loss</code>(<strong><code>hazard</code></strong>:<code>Tuple</code>[<code>Tensor</code>, <code>Tensor</code>], <strong><code>e</code></strong>:<code>Tensor</code>)</p>
</blockquote>
<p>parameters:</p>
<ul>
<li>hazard: log hazard and Cumulative hazard</li>
<li>e: tensor of 1 if death event occured and 0 otherwise</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

